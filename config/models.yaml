# Spotigo 2.0 - AI Model Configuration
# All models are local-only via Ollama

models:
  # Primary chat model - IBM Granite 4.0 (excellent for agentic tasks)
  chat:
    primary: granite4:1b
    fallback: qwen3:0.6b
    description: "Main conversational AI for music discussions"
    max_tokens: 4096
    temperature: 0.7

  # Fast model for simple tasks (classifications, yes/no, formatting)
  fast:
    primary: granite4:350m
    fallback: qwen3:0.6b
    description: "Quick responses for simple operations"
    max_tokens: 1024
    temperature: 0.3

  # Reasoning model for complex analysis
  reasoning:
    primary: qwen3:1.7b
    fallback: granite4:1b
    description: "Deep analysis and complex reasoning tasks"
    max_tokens: 8192
    temperature: 0.5

  # Function calling specialist
  tools:
    primary: functiongemma
    fallback: granite4:1b
    description: "Tool/function calling and structured outputs"
    max_tokens: 2048
    temperature: 0.1

  # Embedding models for RAG
  embeddings:
    primary: nomic-embed-text-v2-moe
    fallback: qwen3-embedding:0.6b
    description: "Semantic embeddings for vector search"
    dimensions: 768

# Agent configurations - which model each agent uses
agents:
  chat_agent:
    model_role: chat
    system_prompt: |
      You are Spotigo, a friendly music intelligence assistant.
      You help users explore their Spotify library, discover insights about their listening habits,
      and find new music. Be conversational, knowledgeable about music, and helpful.
      Always be concise but informative.

  search_agent:
    model_role: fast
    system_prompt: |
      You are a search query optimizer for music data.
      Convert natural language queries into structured search parameters.
      Output JSON with fields: query, filters, sort, limit.

  stats_agent:
    model_role: reasoning
    system_prompt: |
      You are a music statistics analyst.
      Analyze listening data and provide insights about patterns, trends, and preferences.
      Use data to support your observations. Be precise with numbers.

  insight_agent:
    model_role: reasoning
    system_prompt: |
      You are a music taste analyst and recommendation expert.
      Provide deep insights about musical preferences, genre evolution, and artist relationships.
      Make connections between different aspects of the user's music library.

  router_agent:
    model_role: fast
    system_prompt: |
      You are a request router. Classify user requests into categories:
      - chat: General conversation about music
      - search: Looking for specific songs, artists, playlists
      - stats: Requesting statistics or numbers
      - insight: Asking for analysis or recommendations
      - backup: Managing library backups
      - help: Asking for help with the app
      Respond with only the category name.

# Model selection strategy
strategy:
  # Use fast model for initial routing
  routing: fast
  
  # Escalation: if fast model confidence < threshold, use reasoning
  escalation_threshold: 0.7
  
  # Max retries before falling back
  max_retries: 2
  
  # Timeout per request (seconds)
  timeout: 30

# Ollama connection settings
ollama:
  host: "http://localhost:11434"
  # For Docker: host: "http://ollama:11434"
